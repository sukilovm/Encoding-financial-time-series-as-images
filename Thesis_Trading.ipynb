{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "mIN9smnpSvhL"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "mIN9smnpSvhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and Import Libraries\n",
        "%%capture\n",
        "!pip install yfinance\n",
        "!pip install pyts\n",
        "!pip install mplfinance\n",
        "!pip install opencv-python-headless\n",
        "!pip installpandas_market_calendars\n",
        "!pip install tqdm\n",
        "from keras.models import load_model\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import mplfinance as mpf\n",
        "import datetime\n",
        "import logging\n",
        "import warnings\n",
        "import pyts\n",
        "import pickle\n",
        "import pywt\n",
        "import gc\n",
        "import time\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from io import BytesIO\n",
        "from pyts.image import MarkovTransitionField\n",
        "from pyts.image import GramianAngularField\n",
        "from pyts.image import RecurrencePlot\n",
        "from scipy.signal import spectrogram\n",
        "from skimage.measure import block_reduce\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from keras.losses import categorical_crossentropy\n",
        "import warnings\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"Some quantiles are equal.\")\n",
        "warnings.filterwarnings('ignore', message='The frame.append method is deprecated')"
      ],
      "metadata": {
        "id": "ajT1wz_6Fq0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4z5wjsdxrGQ",
        "outputId": "09260b24-3a89-4683-e2cb-ce93d048dd78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Time Series (RAW)"
      ],
      "metadata": {
        "id": "ZJjRAp7sS0Fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code segments generate the S&P 500 index ETF time series windows and labels. <br> In the thesis paper, this data is referred to as the `Index Dataset`."
      ],
      "metadata": {
        "id": "9U3IF5WNnrkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query Index ETF Data**"
      ],
      "metadata": {
        "id": "_QlBcy68wJft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Get $VOO Data\n",
        "ticker_obj = yf.Ticker('VOO')\n",
        "sp500_index_data = ticker_obj.history(start='2016-12-05', end='2022-12-30')\n",
        "\n",
        "# Reset the index\n",
        "sp500_index_data.reset_index(inplace=True)\n",
        "\n",
        "# Extract the year from the 'Date' column and create a new 'Year' column\n",
        "sp500_index_data['Year'] = sp500_index_data['Date'].dt.year\n",
        "\n",
        "# Remove time components\n",
        "sp500_index_data['Date'] = pd.to_datetime(sp500_index_data['Date']).dt.date\n",
        "\n",
        "# Select only the desired columns\n",
        "desired_columns = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Year\"]\n",
        "sp500_index_data = sp500_index_data[desired_columns]\n",
        "\n",
        "# Extract the year from the 'Date' column and create a new 'Year' column\n",
        "\n",
        "## Fill missing trading holidays with previous available day's data\n",
        "\n",
        "# Create a date range between the min and max date, including only weekdays\n",
        "date_range = pd.date_range(start='2016-12-05', end='2022-12-30', freq='B')  # 'B' is for business days (Monday-Friday)\n",
        "\n",
        "# Merge the sp500_index_data with the date_range, forward filling the missing data\n",
        "sp500_index_data = sp500_index_data.set_index('Date').reindex(date_range, method='ffill').reset_index().rename(columns={'index': 'Date'})\n",
        "\n",
        "sp500_index_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "c-Requ7aQrYK",
        "outputId": "3c870743-f698-4962-cb09-5d3240daf7a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Date        Open        High         Low       Close  Year\n",
              "0    2016-12-05  180.158830  180.745612  179.963236  180.398880  2016\n",
              "1    2016-12-06  180.630017  181.038990  180.158817  181.030090  2016\n",
              "2    2016-12-07  180.878972  183.430579  180.745619  183.323883  2016\n",
              "3    2016-12-08  183.368296  184.275140  183.101576  183.803940  2016\n",
              "4    2016-12-09  184.052921  184.950865  184.017351  184.924194  2016\n",
              "...         ...         ...         ...         ...         ...   ...\n",
              "1580 2022-12-26  347.499208  350.506844  345.925700  350.427155  2022\n",
              "1581 2022-12-27  350.317617  350.596467  347.419546  349.032898  2022\n",
              "1582 2022-12-28  348.983122  350.755822  344.481637  344.750549  2022\n",
              "1583 2022-12-29  347.479304  351.682001  347.041106  350.865356  2022\n",
              "1584 2022-12-30  347.479304  351.682001  347.041106  350.865356  2022\n",
              "\n",
              "[1585 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4593da23-d735-49f0-a28f-d67f45743c13\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-12-05</td>\n",
              "      <td>180.158830</td>\n",
              "      <td>180.745612</td>\n",
              "      <td>179.963236</td>\n",
              "      <td>180.398880</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-12-06</td>\n",
              "      <td>180.630017</td>\n",
              "      <td>181.038990</td>\n",
              "      <td>180.158817</td>\n",
              "      <td>181.030090</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-12-07</td>\n",
              "      <td>180.878972</td>\n",
              "      <td>183.430579</td>\n",
              "      <td>180.745619</td>\n",
              "      <td>183.323883</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-12-08</td>\n",
              "      <td>183.368296</td>\n",
              "      <td>184.275140</td>\n",
              "      <td>183.101576</td>\n",
              "      <td>183.803940</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-12-09</td>\n",
              "      <td>184.052921</td>\n",
              "      <td>184.950865</td>\n",
              "      <td>184.017351</td>\n",
              "      <td>184.924194</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1580</th>\n",
              "      <td>2022-12-26</td>\n",
              "      <td>347.499208</td>\n",
              "      <td>350.506844</td>\n",
              "      <td>345.925700</td>\n",
              "      <td>350.427155</td>\n",
              "      <td>2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1581</th>\n",
              "      <td>2022-12-27</td>\n",
              "      <td>350.317617</td>\n",
              "      <td>350.596467</td>\n",
              "      <td>347.419546</td>\n",
              "      <td>349.032898</td>\n",
              "      <td>2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1582</th>\n",
              "      <td>2022-12-28</td>\n",
              "      <td>348.983122</td>\n",
              "      <td>350.755822</td>\n",
              "      <td>344.481637</td>\n",
              "      <td>344.750549</td>\n",
              "      <td>2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1583</th>\n",
              "      <td>2022-12-29</td>\n",
              "      <td>347.479304</td>\n",
              "      <td>351.682001</td>\n",
              "      <td>347.041106</td>\n",
              "      <td>350.865356</td>\n",
              "      <td>2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1584</th>\n",
              "      <td>2022-12-30</td>\n",
              "      <td>347.479304</td>\n",
              "      <td>351.682001</td>\n",
              "      <td>347.041106</td>\n",
              "      <td>350.865356</td>\n",
              "      <td>2022</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1585 rows Ã— 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4593da23-d735-49f0-a28f-d67f45743c13')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4593da23-d735-49f0-a28f-d67f45743c13 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4593da23-d735-49f0-a28f-d67f45743c13');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Raw Time Series (RAW)**"
      ],
      "metadata": {
        "id": "GKb03K-Cv8Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_windows_and_labels(df, window_size, threshold=0.5):\n",
        "    df = df.sort_values(by=['Date'])\n",
        "\n",
        "    windows = []\n",
        "    labels = []\n",
        "    returns = []   # list to store returns\n",
        "    max_years = []\n",
        "\n",
        "    for i in range(len(df) - window_size):\n",
        "        window_start = df.iloc[i]['Date']\n",
        "\n",
        "        # Check if the window starts on a Monday\n",
        "        if window_start.weekday() == 0:\n",
        "            window_end = df.iloc[i + window_size - 1]\n",
        "            next_week_index = i + window_size + 4\n",
        "\n",
        "            # Check if next_week_index is within the dataframe's length\n",
        "            if next_week_index < len(df):\n",
        "                next_week_close = df.iloc[next_week_index]['Close']\n",
        "                window = df.iloc[i:i + window_size][['Date', 'Close', 'Year', 'Open', 'High', 'Low']]\n",
        "                label = 1 if next_week_close > window_end['Close'] else 0\n",
        "\n",
        "                # Calculate the return\n",
        "                return_val = (next_week_close - window_end['Close'])/window_end['Close']\n",
        "\n",
        "                # Check if all the 'Close' values in the window are the same\n",
        "                if window['Close'].nunique() > 1:\n",
        "                    # Calculate the percentage change between consecutive 'Close' values\n",
        "                    window['Close_pct_change'] = window['Close'].pct_change()\n",
        "\n",
        "                    # Check if there is any extreme percentage change (above the threshold)\n",
        "                    extreme_change = any(window['Close_pct_change'].abs() > threshold)\n",
        "\n",
        "                    if not extreme_change:\n",
        "                        window = window.drop(columns=['Close_pct_change'])\n",
        "                        windows.append(window)\n",
        "                        labels.append(label)\n",
        "                        returns.append(return_val)  # Append the return\n",
        "                        max_years.append(window['Year'].max())\n",
        "\n",
        "    return windows, labels, returns, max_years\n",
        "\n",
        "window_size = 20\n",
        "sp500_windows, sp500_labels, sp500_returns, sp500_max_years = create_windows_and_labels(sp500_index_data, window_size)\n",
        "\n",
        "sp500_raw_time_series_arrays = [window['Close'].values for window in sp500_windows]"
      ],
      "metadata": {
        "id": "kTzwNP8lSe3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode Time Series (RAW) as Images"
      ],
      "metadata": {
        "id": "kDo0lPAaZyLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code segments encode the previously created raw time series (RAW) windows as images."
      ],
      "metadata": {
        "id": "GtQRAb7zyYWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Candlestick Charts (CND)**"
      ],
      "metadata": {
        "id": "AMkRVymswO0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_greyscale_conversion(img):\n",
        "    red_channel = img[:, :, 2]\n",
        "    green_channel = img[:, :, 1]\n",
        "    blue_channel = img[:, :, 0]\n",
        "\n",
        "    # Convert the image to greyscale using OpenCV's built-in function\n",
        "    grey_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Create masks to extract red and green candles\n",
        "    red_mask = (red_channel > green_channel) & (blue_channel < red_channel)\n",
        "    green_mask = (green_channel > red_channel) & (blue_channel < green_channel)\n",
        "\n",
        "    # Create a custom greyscale image by emphasizing the difference between red and green channels\n",
        "    custom_grey_img = grey_img.copy()\n",
        "    custom_grey_img[red_mask] = grey_img[red_mask] * 1.5\n",
        "    custom_grey_img[green_mask] = grey_img[green_mask] * 0.5\n",
        "\n",
        "    # Clip the custom greyscale image to the range [0, 255]\n",
        "    custom_grey_img = np.clip(custom_grey_img, 0, 255)\n",
        "\n",
        "    return custom_grey_img.astype(np.uint8)\n",
        "\n",
        "def max_pooling(img, pool_size):\n",
        "    return block_reduce(img, block_size=(pool_size, pool_size), func=np.max)\n",
        "\n",
        "def generate_candlestick_array(window):\n",
        "    target_size = (20, 20)\n",
        "    pool_size = 2\n",
        "\n",
        "    num_rows = window.shape[0]\n",
        "    temp_index = pd.date_range(start='2000-01-01', periods=num_rows, freq='D')\n",
        "    temp_window = window.copy()\n",
        "    temp_window.index = temp_index\n",
        "\n",
        "    fig, ax = mpf.plot(temp_window, type='candle', style='charles', returnfig=True, axisoff=True)\n",
        "\n",
        "    buf = BytesIO()\n",
        "    fig.savefig(buf, format='png', bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    buf.seek(0)\n",
        "    img = cv2.imdecode(np.frombuffer(buf.read(), np.uint8), -1)\n",
        "\n",
        "    grey_img = custom_greyscale_conversion(img)\n",
        "\n",
        "    pooled_grey_img = max_pooling(grey_img, pool_size)\n",
        "\n",
        "    resized_grey_img = cv2.resize(pooled_grey_img, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    return resized_grey_img\n",
        "\n",
        "sp500_candlestick_arrays = []\n",
        "\n",
        "for window in sp500_windows:\n",
        "  sp500_candlestick_array = generate_candlestick_array(window)\n",
        "  sp500_candlestick_arrays.append(sp500_candlestick_array)\n",
        "  del sp500_candlestick_array"
      ],
      "metadata": {
        "id": "7_D_xuxfReq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markov Transition Fields (MTF)**"
      ],
      "metadata": {
        "id": "KFOi9L-3wWpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_mtf_arrays(windows, n_bins=5, size=(20, 20)):\n",
        "    images = []\n",
        "    mtf = MarkovTransitionField(n_bins=n_bins)\n",
        "\n",
        "    for window in windows:\n",
        "        # Extract the 'Close' column from the window\n",
        "        close_prices = window['Close'].values\n",
        "\n",
        "        # Compute the MTF\n",
        "        mtf_image = mtf.fit_transform([close_prices])[0]\n",
        "\n",
        "        # Normalize the MTF image to the range [0, 1]\n",
        "        mtf_image_normalized = (mtf_image - mtf_image.min()) / (mtf_image.max() - mtf_image.min())\n",
        "\n",
        "        # Convert the normalized MTF image to a grayscale image\n",
        "        mtf_image_grayscale = (mtf_image_normalized * 255).astype(np.uint8)\n",
        "\n",
        "        # Resize the MTF image\n",
        "        mtf_image_resized = cv2.resize(mtf_image_grayscale, size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        # Append the image to the list of images\n",
        "        images.append(mtf_image_resized)\n",
        "\n",
        "    return images\n",
        "\n",
        "sp500_mtf_arrays = generate_mtf_arrays(sp500_windows)"
      ],
      "metadata": {
        "id": "zfL9iC_GSeTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gramian Angular Fields (GAF)**"
      ],
      "metadata": {
        "id": "2jXlfqxQwZto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_gaf_difference_arrays(windows, method='difference', size=(20, 20)):\n",
        "    arrays = []\n",
        "    gaf = GramianAngularField(method=method)\n",
        "\n",
        "    for window in windows:\n",
        "        close_prices = window['Close'].values\n",
        "        gaf_image = gaf.fit_transform([close_prices])[0]\n",
        "        gaf_image_normalized = (gaf_image - gaf_image.min()) / (gaf_image.max() - gaf_image.min())\n",
        "\n",
        "        # Convert the normalized GAF image to a greyscale image\n",
        "        gaf_image_grayscale = (gaf_image_normalized * 255).astype(np.uint8)\n",
        "\n",
        "        # Resize the greyscale GAF image\n",
        "        gaf_image_resized = cv2.resize(gaf_image_grayscale, size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        arrays.append(gaf_image_resized)  # Append the resized greyscale array to the list of arrays\n",
        "\n",
        "    return arrays\n",
        "\n",
        "sp500_gaf_arrays = generate_gaf_difference_arrays(sp500_windows)"
      ],
      "metadata": {
        "id": "Uv-vG6EDSfON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split Data**"
      ],
      "metadata": {
        "id": "RIEu0_85v0wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the main dictionary\n",
        "sp500_dict = {2017: {}, 2018: {}, 2019: {}, 2020: {}, 2021: {}, 2022: {}}\n",
        "\n",
        "# Initialize the nested dictionaries\n",
        "for year in sp500_dict.keys():\n",
        "    sp500_dict[year] = {\n",
        "        'labels': [],\n",
        "        'weekly_returns': [],\n",
        "        'raw_time_series': [],\n",
        "        'candlestick': [],\n",
        "        'mtf': [],\n",
        "        'gaf': []\n",
        "    }\n",
        "\n",
        "# Iterate through all the arrays simultaneously\n",
        "for max_year, label, weekly_return, raw_time_series, candlestick, mtf, gaf in zip(\n",
        "    sp500_max_years,\n",
        "    sp500_labels,\n",
        "    sp500_returns,\n",
        "    sp500_raw_time_series_arrays,\n",
        "    sp500_candlestick_arrays,\n",
        "    sp500_mtf_arrays,\n",
        "    sp500_gaf_arrays):\n",
        "\n",
        "    # Determine the year to assign the data to\n",
        "    if max_year <= 2017:\n",
        "        year = 2017\n",
        "    elif max_year == 2018:\n",
        "        year = 2018\n",
        "    elif max_year == 2019:\n",
        "        year = 2019\n",
        "    elif max_year == 2020:\n",
        "        year = 2020\n",
        "    elif max_year == 2021:\n",
        "        year = 2021\n",
        "    elif max_year == 2022:\n",
        "        year = 2022\n",
        "\n",
        "    # Add the data to the corresponding year in the dictionary\n",
        "    sp500_dict[year]['labels'].append(label)\n",
        "    sp500_dict[year]['weekly_returns'].append(weekly_return)\n",
        "    sp500_dict[year]['raw_time_series'].append(raw_time_series)\n",
        "    sp500_dict[year]['candlestick'].append(candlestick)\n",
        "    sp500_dict[year]['mtf'].append(mtf)\n",
        "    sp500_dict[year]['gaf'].append(gaf)"
      ],
      "metadata": {
        "id": "LRQ8ge7lZYmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reshape Data**"
      ],
      "metadata": {
        "id": "pTIsJRSweyvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp500_dict_reshaped = {}\n",
        "\n",
        "for year, data in sp500_dict.items():\n",
        "    reshaped_data = {}\n",
        "\n",
        "    for key, array_data in data.items():\n",
        "        if key == 'raw_time_series':\n",
        "            reshaped_data_array = [array.reshape(20, 1, 1) for array in array_data]\n",
        "        elif key in ['candlestick', 'mtf', 'gaf']:\n",
        "            reshaped_data_array = [array.reshape(20, 20, 1) for array in array_data]\n",
        "        else:  # For 'labels' and 'weekly_returns', no reshaping is needed\n",
        "            reshaped_data_array = array_data\n",
        "\n",
        "        reshaped_data[key] = reshaped_data_array\n",
        "\n",
        "    sp500_dict_reshaped[year] = reshaped_data"
      ],
      "metadata": {
        "id": "-gRof_MVewH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Fitted Models and Predict (Index Dataset)"
      ],
      "metadata": {
        "id": "3mqpbk-Ps2UT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code segments load the models fitted on the `Constituents Dataset` in the previous notebook `Thesis_Models`. <br> These are used to generate predictions on the `Index Dataset`.\n",
        "<br> <br> The models, which achieved minimum, median, and maximum prediction accuracy on the `Constituents Dataset`, <br> have been loaded and tested on the `Index Dataset`. The code output of the following cells shows the results of the maximum models."
      ],
      "metadata": {
        "id": "GFT2pB9fpfSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Raw Time Series (RAW)**"
      ],
      "metadata": {
        "id": "Z37q_4h_0GtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load models fitted on the Constituents Dataset\n",
        "RAW_model_2017 = load_model(f'/content/drive/My Drive/X2_raw_time_series_model_period_1.h5')\n",
        "RAW_model_2018 = load_model(f'/content/drive/My Drive/X4_raw_time_series_model_period_2.h5')\n",
        "RAW_model_2019 = load_model(f'/content/drive/My Drive/X3_raw_time_series_model_period_3.h5')\n",
        "RAW_model_2020 = load_model(f'/content/drive/My Drive/X4_raw_time_series_model_period_4.h5')\n",
        "RAW_model_2021 = load_model(f'/content/drive/My Drive/X2_raw_time_series_model_period_5.h5')\n",
        "RAW_model_2022 = load_model(f'/content/drive/My Drive/X2_raw_time_series_model_period_6.h5')\n",
        "\n",
        "# List of models and corresponding years\n",
        "models = [(RAW_model_2017, 2017), (RAW_model_2018, 2018), (RAW_model_2019, 2019),\n",
        "          (RAW_model_2020, 2020), (RAW_model_2021, 2021), (RAW_model_2022, 2022)]\n",
        "\n",
        "## Use fitted models for predictions on the Index Dataset\n",
        "# Iterate through each model and year\n",
        "for model, year in models:\n",
        "    # Get raw time series data and labels\n",
        "    raw_time_series_data = np.array(sp500_dict_reshaped[year]['raw_time_series'])\n",
        "    actual_labels = sp500_dict_reshaped[year]['labels']\n",
        "\n",
        "    # Convert labels to one-hot encoded labels\n",
        "    actual_labels_one_hot = to_categorical(actual_labels)\n",
        "\n",
        "    # Predict the probabilities\n",
        "    predicted_probabilities = model.predict(raw_time_series_data)\n",
        "\n",
        "    # Get the class with the highest probability\n",
        "    predicted_labels = np.argmax(predicted_probabilities, axis=-1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(actual_labels, predicted_labels)\n",
        "\n",
        "    # Evaluate the loss\n",
        "    loss = model.evaluate(raw_time_series_data, actual_labels_one_hot, verbose=0)[0]\n",
        "\n",
        "    print(f\"Raw Time Series Model for year {year} Loss: {loss}\")\n",
        "    print(f\"Raw Time Series Model for year {year} Accuracy: {accuracy}\")\n",
        "    print(f\"Raw Time Series Model for year {year} Labels: {', '.join(map(str, predicted_labels))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtiWecy3d3L0",
        "outputId": "e003091d-58fa-47c7-ac17-30752d1d2313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 71ms/step\n",
            "Raw Time Series Model for year 2017 Loss: 0.6566095352172852\n",
            "Raw Time Series Model for year 2017 Accuracy: 0.6792452830188679\n",
            "Raw Time Series Model for year 2017 Labels: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
            "2/2 [==============================] - 0s 64ms/step\n",
            "Raw Time Series Model for year 2018 Loss: 0.6826632618904114\n",
            "Raw Time Series Model for year 2018 Accuracy: 0.5769230769230769\n",
            "Raw Time Series Model for year 2018 Labels: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
            "2/2 [==============================] - 0s 64ms/step\n",
            "Raw Time Series Model for year 2019 Loss: 0.6624692678451538\n",
            "Raw Time Series Model for year 2019 Accuracy: 0.6346153846153846\n",
            "Raw Time Series Model for year 2019 Labels: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
            "2/2 [==============================] - 0s 60ms/step\n",
            "Raw Time Series Model for year 2020 Loss: 0.6778696179389954\n",
            "Raw Time Series Model for year 2020 Accuracy: 0.5849056603773585\n",
            "Raw Time Series Model for year 2020 Labels: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdf7e8895a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 57ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7fdf7e888f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Time Series Model for year 2021 Loss: 0.669948935508728\n",
            "Raw Time Series Model for year 2021 Accuracy: 0.6538461538461539\n",
            "Raw Time Series Model for year 2021 Labels: 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdf7e888ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 59ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fdf80c32950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Time Series Model for year 2022 Loss: 0.684952974319458\n",
            "Raw Time Series Model for year 2022 Accuracy: 0.5490196078431373\n",
            "Raw Time Series Model for year 2022 Labels: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Candlestick Charts (CND)**"
      ],
      "metadata": {
        "id": "7VfiLb3ZuQZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load models fitted on the Constituents Dataset\n",
        "CND_model_2017 = load_model(f'/content/drive/My Drive/X5_candlestick_model_period_1.h5')\n",
        "CND_model_2018 = load_model(f'/content/drive/My Drive/X3_candlestick_model_period_2.h5')\n",
        "CND_model_2019 = load_model(f'/content/drive/My Drive/X1_candlestick_model_period_3.h5')\n",
        "CND_model_2020 = load_model(f'/content/drive/My Drive/X2_candlestick_model_period_4.h5')\n",
        "CND_model_2021 = load_model(f'/content/drive/My Drive/X3_candlestick_model_period_5.h5')\n",
        "CND_model_2022 = load_model(f'/content/drive/My Drive/X3_candlestick_model_period_6.h5')\n",
        "\n",
        "# List of models and corresponding years\n",
        "models = [(CND_model_2017, 2017), (CND_model_2018, 2018), (CND_model_2019, 2019),\n",
        "          (CND_model_2020, 2020), (CND_model_2021, 2021), (CND_model_2022, 2022)]\n",
        "\n",
        "## Use fitted models for predictions on the Index Dataset\n",
        "# Iterate through each model and year\n",
        "for model, year in models:\n",
        "    # Get candlestick data and labels\n",
        "    candlestick_data = np.array(sp500_dict_reshaped[year]['candlestick'])\n",
        "    actual_labels = sp500_dict_reshaped[year]['labels']\n",
        "\n",
        "    # Convert labels to one-hot encoded labels\n",
        "    actual_labels_one_hot = to_categorical(actual_labels)\n",
        "\n",
        "    # Predict the probabilities\n",
        "    predicted_probabilities = model.predict(candlestick_data)\n",
        "\n",
        "    # Get the class with the highest probability\n",
        "    predicted_labels = np.argmax(predicted_probabilities, axis=-1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(actual_labels, predicted_labels)\n",
        "\n",
        "    # Evaluate the loss\n",
        "    loss = model.evaluate(candlestick_data, actual_labels_one_hot, verbose=0)[0]\n",
        "\n",
        "    print(f\"Candlestick Model for year {year} Loss: {loss}\")\n",
        "    print(f\"Candlestick Model for year {year} Accuracy: {accuracy}\")\n",
        "    print(f\"Candlestick Model for year {year} Labels: {', '.join(map(str, predicted_labels))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVaYN5jwp58m",
        "outputId": "77091156-ef80-4eef-fc18-125713023f73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 311ms/step\n",
            "Candlestick Model for year 2017 Loss: 0.6585046052932739\n",
            "Candlestick Model for year 2017 Accuracy: 0.5849056603773585\n",
            "Candlestick Model for year 2017 Labels: 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0\n",
            "2/2 [==============================] - 1s 224ms/step\n",
            "Candlestick Model for year 2018 Loss: 0.7596337795257568\n",
            "Candlestick Model for year 2018 Accuracy: 0.5576923076923077\n",
            "Candlestick Model for year 2018 Labels: 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1\n",
            "2/2 [==============================] - 1s 382ms/step\n",
            "Candlestick Model for year 2019 Loss: 0.7013486623764038\n",
            "Candlestick Model for year 2019 Accuracy: 0.6346153846153846\n",
            "Candlestick Model for year 2019 Labels: 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0\n",
            "2/2 [==============================] - 1s 210ms/step\n",
            "Candlestick Model for year 2020 Loss: 0.667930006980896\n",
            "Candlestick Model for year 2020 Accuracy: 0.6037735849056604\n",
            "Candlestick Model for year 2020 Labels: 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1\n",
            "2/2 [==============================] - 1s 202ms/step\n",
            "Candlestick Model for year 2021 Loss: 0.6544017791748047\n",
            "Candlestick Model for year 2021 Accuracy: 0.5961538461538461\n",
            "Candlestick Model for year 2021 Labels: 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1\n",
            "2/2 [==============================] - 1s 195ms/step\n",
            "Candlestick Model for year 2022 Loss: 0.739665687084198\n",
            "Candlestick Model for year 2022 Accuracy: 0.5490196078431373\n",
            "Candlestick Model for year 2022 Labels: 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markov Transition Fields (MTF)**"
      ],
      "metadata": {
        "id": "fFYVrnHro0-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models fitted on the Constituents Dataset\n",
        "MTF_model_2017 = load_model(f'/content/drive/My Drive/X5_mtf_model_period_1.h5')\n",
        "MTF_model_2018 = load_model(f'/content/drive/My Drive/X3_mtf_model_period_2.h5')\n",
        "MTF_model_2019 = load_model(f'/content/drive/My Drive/X5_mtf_model_period_3.h5')\n",
        "MTF_model_2020 = load_model(f'/content/drive/My Drive/X3_mtf_model_period_4.h5')\n",
        "MTF_model_2021 = load_model(f'/content/drive/My Drive/X2_mtf_model_period_5.h5')\n",
        "MTF_model_2022 = load_model(f'/content/drive/My Drive/X1_mtf_model_period_6.h5')\n",
        "\n",
        "# List of models and corresponding years\n",
        "models = [(MTF_model_2017, 2017), (MTF_model_2018, 2018), (MTF_model_2019, 2019),\n",
        "          (MTF_model_2020, 2020), (MTF_model_2021, 2021), (MTF_model_2022, 2022)]\n",
        "\n",
        "## Use fitted models for predictions on the Index Dataset\n",
        "# Iterate through each model and year\n",
        "for model, year in models:\n",
        "    # Get MTF data and labels\n",
        "    mtf_data = np.array(sp500_dict_reshaped[year]['mtf'])\n",
        "    actual_labels = sp500_dict_reshaped[year]['labels']\n",
        "\n",
        "    # Convert labels to one-hot encoded labels\n",
        "    actual_labels_one_hot = to_categorical(actual_labels)\n",
        "\n",
        "    # Predict the probabilities\n",
        "    predicted_probabilities = model.predict(mtf_data)\n",
        "\n",
        "    # Get the class with the highest probability\n",
        "    predicted_labels = np.argmax(predicted_probabilities, axis=-1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(actual_labels, predicted_labels)\n",
        "\n",
        "    # Evaluate the loss\n",
        "    loss = model.evaluate(mtf_data, actual_labels_one_hot, verbose=0)[0]\n",
        "\n",
        "    print(f\"MTF Model for year {year} Loss: {loss}\")\n",
        "    print(f\"MTF Model for year {year} Accuracy: {accuracy}\")\n",
        "    print(f\"MTF Model for year {year} Labels: {', '.join(map(str, predicted_labels))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IzBh5v92A8y",
        "outputId": "c271389c-6b9c-4f4e-ae9e-64f5e59d6d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 207ms/step\n",
            "MTF Model for year 2017 Loss: 0.6509992480278015\n",
            "MTF Model for year 2017 Accuracy: 0.6226415094339622\n",
            "MTF Model for year 2017 Labels: 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1\n",
            "2/2 [==============================] - 1s 197ms/step\n",
            "MTF Model for year 2018 Loss: 0.7190297245979309\n",
            "MTF Model for year 2018 Accuracy: 0.5576923076923077\n",
            "MTF Model for year 2018 Labels: 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1\n",
            "2/2 [==============================] - 1s 233ms/step\n",
            "MTF Model for year 2019 Loss: 0.7530075907707214\n",
            "MTF Model for year 2019 Accuracy: 0.5384615384615384\n",
            "MTF Model for year 2019 Labels: 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1\n",
            "2/2 [==============================] - 1s 242ms/step\n",
            "MTF Model for year 2020 Loss: 0.6557144522666931\n",
            "MTF Model for year 2020 Accuracy: 0.6037735849056604\n",
            "MTF Model for year 2020 Labels: 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0\n",
            "2/2 [==============================] - 1s 202ms/step\n",
            "MTF Model for year 2021 Loss: 0.7144502997398376\n",
            "MTF Model for year 2021 Accuracy: 0.6153846153846154\n",
            "MTF Model for year 2021 Labels: 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0\n",
            "2/2 [==============================] - 1s 193ms/step\n",
            "MTF Model for year 2022 Loss: 0.7117745876312256\n",
            "MTF Model for year 2022 Accuracy: 0.5686274509803921\n",
            "MTF Model for year 2022 Labels: 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gramian Angular Fields (GAF)**"
      ],
      "metadata": {
        "id": "7NW0UfsdovIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load models fitted on the Constituents Dataset\n",
        "GAF_model_2017 = load_model(f'/content/drive/My Drive/X2_gaf_model_period_1.h5')\n",
        "GAF_model_2018 = load_model(f'/content/drive/My Drive/X5_gaf_model_period_2.h5')\n",
        "GAF_model_2019 = load_model(f'/content/drive/My Drive/X1_gaf_model_period_3.h5')\n",
        "GAF_model_2020 = load_model(f'/content/drive/My Drive/X1_gaf_model_period_4.h5')\n",
        "GAF_model_2021 = load_model(f'/content/drive/My Drive/X1_gaf_model_period_5.h5')\n",
        "GAF_model_2022 = load_model(f'/content/drive/My Drive/X5_gaf_model_period_6.h5')\n",
        "\n",
        "# List of models and corresponding years\n",
        "models = [(GAF_model_2017, 2017), (GAF_model_2018, 2018), (GAF_model_2019, 2019),\n",
        "          (GAF_model_2020, 2020), (GAF_model_2021, 2021), (GAF_model_2022, 2022)]\n",
        "\n",
        "## Use fitted models for predictions on the Index Dataset\n",
        "# Iterate through each model and year\n",
        "for model, year in models:\n",
        "    # Get GAF data and labels\n",
        "    gaf_data = np.array(sp500_dict_reshaped[year]['gaf'])\n",
        "    actual_labels = sp500_dict_reshaped[year]['labels']\n",
        "\n",
        "    # Convert labels to one-hot encoded labels\n",
        "    actual_labels_one_hot = to_categorical(actual_labels)\n",
        "\n",
        "    # Predict the probabilities\n",
        "    predicted_probabilities = model.predict(gaf_data)\n",
        "\n",
        "    # Get the class with the highest probability\n",
        "    predicted_labels = np.argmax(predicted_probabilities, axis=-1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(actual_labels, predicted_labels)\n",
        "\n",
        "    # Evaluate the loss\n",
        "    loss = model.evaluate(gaf_data, actual_labels_one_hot, verbose=0)[0]\n",
        "\n",
        "    print(f\"GAF Model for year {year} Loss: {loss}\")\n",
        "    print(f\"GAF Model for year {year} Accuracy: {accuracy}\")\n",
        "    print(f\"GAF Model for year {year} Labels: {', '.join(map(str, predicted_labels))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwJ5-rl85Ygr",
        "outputId": "48fae1ac-9379-4534-c122-2aa9cdf653f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 346ms/step\n",
            "GAF Model for year 2017 Loss: 0.7184849977493286\n",
            "GAF Model for year 2017 Accuracy: 0.6037735849056604\n",
            "GAF Model for year 2017 Labels: 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1\n",
            "2/2 [==============================] - 1s 232ms/step\n",
            "GAF Model for year 2018 Loss: 0.7542997598648071\n",
            "GAF Model for year 2018 Accuracy: 0.5192307692307693\n",
            "GAF Model for year 2018 Labels: 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0\n",
            "2/2 [==============================] - 1s 198ms/step\n",
            "GAF Model for year 2019 Loss: 0.811617910861969\n",
            "GAF Model for year 2019 Accuracy: 0.5384615384615384\n",
            "GAF Model for year 2019 Labels: 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
            "2/2 [==============================] - 1s 208ms/step\n",
            "GAF Model for year 2020 Loss: 0.6849138736724854\n",
            "GAF Model for year 2020 Accuracy: 0.6792452830188679\n",
            "GAF Model for year 2020 Labels: 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1\n",
            "2/2 [==============================] - 2s 520ms/step\n",
            "GAF Model for year 2021 Loss: 0.7566598653793335\n",
            "GAF Model for year 2021 Accuracy: 0.5576923076923077\n",
            "GAF Model for year 2021 Labels: 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1\n",
            "2/2 [==============================] - 2s 469ms/step\n",
            "GAF Model for year 2022 Loss: 0.7228884696960449\n",
            "GAF Model for year 2022 Accuracy: 0.5098039215686274\n",
            "GAF Model for year 2022 Labels: 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Moving Average (SMA)**"
      ],
      "metadata": {
        "id": "hRSNwLx-ojGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Simple moving average calculation on the Index Dataset\n",
        "def average_calculation(raw_time_series_arrays, labels):\n",
        "    average_values = []\n",
        "    average_labels = []\n",
        "\n",
        "    for raw_time_series in raw_time_series_arrays:\n",
        "        if raw_time_series.shape[0] != 20:\n",
        "            raise ValueError(\"Each array in the input list must contain exactly 20 elements.\")\n",
        "\n",
        "        # Calculate the average for all 20 elements\n",
        "        avg_value = np.mean(raw_time_series, axis=0)\n",
        "\n",
        "        # Determine the label (1 if the 20th element of the time series is > than the average, 0 otherwise)\n",
        "        label = 1 if raw_time_series[-1] > avg_value else 0\n",
        "\n",
        "        average_values.append(avg_value)\n",
        "        average_labels.append(label)\n",
        "\n",
        "    # Calculate accuracy for the period\n",
        "    accuracy = accuracy_score(labels, average_labels)\n",
        "\n",
        "    return average_values, average_labels, accuracy\n",
        "\n",
        "def process_periods_avg(sp500_dict_reshaped):\n",
        "    period_dict = {}\n",
        "\n",
        "    for year in range(2017, 2023):\n",
        "        # Get raw time series data and labels\n",
        "        raw_time_series_data = np.array(sp500_dict_reshaped[year]['raw_time_series'])\n",
        "        labels = sp500_dict_reshaped[year]['labels']\n",
        "\n",
        "        avg_values, avg_labels, accuracy = average_calculation(raw_time_series_data, labels)\n",
        "\n",
        "        period_dict[year] = {\n",
        "            'average_values': avg_values,\n",
        "            'average_labels': avg_labels,\n",
        "            'accuracy': accuracy,\n",
        "        }\n",
        "\n",
        "    return period_dict\n",
        "\n",
        "all_periods_average_data = process_periods_avg(sp500_dict_reshaped)\n",
        "\n",
        "for year, period_data in all_periods_average_data.items():\n",
        "    print(f\"Year {year} Average-based Model Accuracy: {period_data['accuracy']}\")\n",
        "    print(f\"Year {year} Average-based Model Labels: {', '.join(map(str, period_data['average_labels']))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9Parlq9Pa8P",
        "outputId": "9a283b48-3f01-4192-a38b-0627a6f788e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Year 2017 Average-based Model Accuracy: 0.5660377358490566\n",
            "Year 2017 Average-based Model Labels: 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
            "Year 2018 Average-based Model Accuracy: 0.46153846153846156\n",
            "Year 2018 Average-based Model Labels: 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0\n",
            "Year 2019 Average-based Model Accuracy: 0.6923076923076923\n",
            "Year 2019 Average-based Model Labels: 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
            "Year 2020 Average-based Model Accuracy: 0.5471698113207547\n",
            "Year 2020 Average-based Model Labels: 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
            "Year 2021 Average-based Model Accuracy: 0.4807692307692308\n",
            "Year 2021 Average-based Model Labels: 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1\n",
            "Year 2022 Average-based Model Accuracy: 0.5882352941176471\n",
            "Year 2022 Average-based Model Labels: 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Prediction (RND)**"
      ],
      "metadata": {
        "id": "S2Yb2nIzor3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Random predictions on the Index Dataset\n",
        "# Initialize random seed for reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "for year in range(2017, 2023):\n",
        "    # Get the labels\n",
        "    actual_labels = sp500_dict[year]['labels']\n",
        "    actual_labels_categorical = to_categorical(actual_labels)  # convert to one-hot vectors\n",
        "\n",
        "    # Generate 5 random predictions\n",
        "    for i in range(5):\n",
        "        # Generate random probabilities\n",
        "        random_predictions = np.random.rand(len(actual_labels), 2)  # Binary classification\n",
        "        random_predictions = random_predictions / np.sum(random_predictions, axis=1, keepdims=True)  # Normalize to make probabilities sum to 1\n",
        "\n",
        "        # Calculate accuracy\n",
        "        predicted_labels = np.argmax(random_predictions, axis=-1)\n",
        "        accuracy = accuracy_score(actual_labels, predicted_labels)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = categorical_crossentropy(actual_labels_categorical, random_predictions).numpy().mean()\n",
        "\n",
        "        print(f\"Year {year} - Random guess {i+1} Loss: {loss}, Accuracy: {accuracy}, Labels: {', '.join(map(str, predicted_labels))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMShgLdA7j5E",
        "outputId": "98ddd99a-b692-4f33-8040-d68bfefe75d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Year 2017 - Random guess 1 Loss: 1.0483529853814881, Accuracy: 0.41509433962264153, Labels: 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1\n",
            "Year 2017 - Random guess 2 Loss: 0.8783584660819423, Accuracy: 0.39622641509433965, Labels: 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0\n",
            "Year 2017 - Random guess 3 Loss: 1.0794107494114413, Accuracy: 0.4339622641509434, Labels: 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1\n",
            "Year 2017 - Random guess 4 Loss: 0.7002639746410375, Accuracy: 0.6415094339622641, Labels: 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0\n",
            "Year 2017 - Random guess 5 Loss: 0.7053601946114849, Accuracy: 0.5849056603773585, Labels: 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1\n",
            "Year 2018 - Random guess 1 Loss: 0.7360429886081306, Accuracy: 0.6346153846153846, Labels: 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1\n",
            "Year 2018 - Random guess 2 Loss: 0.969239064594338, Accuracy: 0.5, Labels: 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1\n",
            "Year 2018 - Random guess 3 Loss: 0.7103453203367249, Accuracy: 0.5384615384615384, Labels: 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0\n",
            "Year 2018 - Random guess 4 Loss: 0.8051244113608443, Accuracy: 0.4807692307692308, Labels: 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0\n",
            "Year 2018 - Random guess 5 Loss: 0.8632007015305483, Accuracy: 0.5192307692307693, Labels: 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1\n",
            "Year 2019 - Random guess 1 Loss: 0.9601391380248442, Accuracy: 0.5961538461538461, Labels: 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1\n",
            "Year 2019 - Random guess 2 Loss: 0.8788428199941092, Accuracy: 0.46153846153846156, Labels: 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1\n",
            "Year 2019 - Random guess 3 Loss: 0.8429276529921975, Accuracy: 0.5769230769230769, Labels: 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1\n",
            "Year 2019 - Random guess 4 Loss: 0.9217563608529538, Accuracy: 0.4230769230769231, Labels: 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0\n",
            "Year 2019 - Random guess 5 Loss: 0.8591531985188265, Accuracy: 0.5384615384615384, Labels: 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1\n",
            "Year 2020 - Random guess 1 Loss: 1.048008324415193, Accuracy: 0.4716981132075472, Labels: 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0\n",
            "Year 2020 - Random guess 2 Loss: 0.8415475854101203, Accuracy: 0.5849056603773585, Labels: 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0\n",
            "Year 2020 - Random guess 3 Loss: 0.8261541148245785, Accuracy: 0.5283018867924528, Labels: 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1\n",
            "Year 2020 - Random guess 4 Loss: 0.7279785313077609, Accuracy: 0.6981132075471698, Labels: 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1\n",
            "Year 2020 - Random guess 5 Loss: 0.9782956657686194, Accuracy: 0.5094339622641509, Labels: 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1\n",
            "Year 2021 - Random guess 1 Loss: 0.8111843883364797, Accuracy: 0.5576923076923077, Labels: 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1\n",
            "Year 2021 - Random guess 2 Loss: 0.8815348383053886, Accuracy: 0.46153846153846156, Labels: 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0\n",
            "Year 2021 - Random guess 3 Loss: 0.8005661945271522, Accuracy: 0.5576923076923077, Labels: 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1\n",
            "Year 2021 - Random guess 4 Loss: 0.8352308338728184, Accuracy: 0.5961538461538461, Labels: 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0\n",
            "Year 2021 - Random guess 5 Loss: 0.7911027240315952, Accuracy: 0.5, Labels: 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0\n",
            "Year 2022 - Random guess 1 Loss: 0.8190852738769326, Accuracy: 0.5098039215686274, Labels: 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0\n",
            "Year 2022 - Random guess 2 Loss: 0.7328981623229838, Accuracy: 0.5686274509803921, Labels: 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1\n",
            "Year 2022 - Random guess 3 Loss: 0.8499679822285102, Accuracy: 0.47058823529411764, Labels: 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0\n",
            "Year 2022 - Random guess 4 Loss: 0.9347483114305828, Accuracy: 0.5686274509803921, Labels: 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0\n",
            "Year 2022 - Random guess 5 Loss: 0.9101662051729043, Accuracy: 0.4117647058823529, Labels: 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1\n"
          ]
        }
      ]
    }
  ]
}