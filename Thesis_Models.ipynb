{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Running this notebook consumes multiple hours and excessive RAM. <br> As a result, file references and function calls are commented out.\n",
        "<br> <br>\n",
        "The code is provided for clarity and understanding.** <br>"
      ],
      "metadata": {
        "id": "6sjZXKpavEqP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh4hTrpgPKYE"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J89o-JBMisha"
      },
      "outputs": [],
      "source": [
        "# Install and Import Libraries\n",
        "%%capture\n",
        "!pip install pmdarima\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import pprint\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import pmdarima as pm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.losses import categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2VBWTojS_VF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1b73e5-8c0c-4b96-fbc3-bba7df303be6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMaDPSXmPMb5"
      },
      "source": [
        "# Load Data (Constituents Dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code segment loads the times series and image encodings generated in the previous notebook **Thesis_Images**. <br> In the thesis paper, these datasets are refered to as the **Constituents Dataset**."
      ],
      "metadata": {
        "id": "PPlFiCREwq-N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77_E5RyBPRFC"
      },
      "outputs": [],
      "source": [
        "# Raw Time Series\n",
        "# with open('/content/drive/MyDrive/20230425_raw_time_series_arrays.pkl', 'rb') as f:\n",
        "#    raw_time_series_arrays = pickle.load(f)\n",
        "\n",
        "# Candlestick images\n",
        "# with open('/content/drive/MyDrive/20230425_candlestick_arrays.pkl', 'rb') as f:\n",
        "#    candlestick_arrays = pickle.load(f)\n",
        "\n",
        "# MTF images\n",
        "# with open('/content/drive/MyDrive/20230425_mtf_arrays.pkl', 'rb') as f:\n",
        "#    mtf_arrays = pickle.load(f)\n",
        "\n",
        "# GAF images\n",
        "# with open('/content/drive/MyDrive/20230425_gaf_arrays.pkl', 'rb') as f:\n",
        "#    gaf_arrays = pickle.load(f)\n",
        "\n",
        "# Labels\n",
        "# with open('/content/drive/MyDrive/20230425_labels.pkl', 'rb') as f:\n",
        "#    labels = pickle.load(f)\n",
        "\n",
        "# Years\n",
        "# with open('/content/drive/MyDrive/20230425_max_years.pkl', 'rb') as f:\n",
        "#    max_years = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNTXF_QumbSI"
      },
      "source": [
        "# Split Datasets (Train, Validation, Test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code segment splits the data in Train, Validation and Test Sets as defined in the thesis paper."
      ],
      "metadata": {
        "id": "xhvJ8IMplRyc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8pscryQmilu"
      },
      "outputs": [],
      "source": [
        "assert len(raw_time_series_arrays) == len(candlestick_arrays) == len(mtf_arrays) == len(gaf_arrays) == len(labels) == len(max_years)\n",
        "\n",
        "# Define the periods\n",
        "periods = [\n",
        "    {'train': (2013, 2015), 'validation': 2016, 'test': 2017},\n",
        "    {'train': (2014, 2016), 'validation': 2017, 'test': 2018},\n",
        "    {'train': (2015, 2017), 'validation': 2018, 'test': 2019},\n",
        "    {'train': (2016, 2018), 'validation': 2019, 'test': 2020},\n",
        "    {'train': (2017, 2019), 'validation': 2020, 'test': 2021},\n",
        "    {'train': (2018, 2020), 'validation': 2021, 'test': 2022},\n",
        "]\n",
        "\n",
        "data_lists = [\n",
        "    raw_time_series_arrays,\n",
        "    candlestick_arrays,\n",
        "    mtf_arrays,\n",
        "    gaf_arrays,\n",
        "    labels,\n",
        "    max_years\n",
        "]\n",
        "\n",
        "data_list_names = [\n",
        "    'raw_time_series',\n",
        "    'candlestick',\n",
        "    'mtf',\n",
        "    'gaf',\n",
        "    'labels',\n",
        "    'max_years'\n",
        "]\n",
        "\n",
        "# Create a nested dictionary to store the train, validation, and test sets for each period\n",
        "data_splits = {i: {'train': {}, 'validation': {}, 'test': {}} for i in range(len(periods))}\n",
        "\n",
        "# Iterate over periods\n",
        "for i, period in enumerate(periods):\n",
        "    train_years = set(range(period['train'][0], period['train'][1] + 1))\n",
        "    validation_year = period['validation']\n",
        "    test_year = period['test']\n",
        "\n",
        "    # Split data into train, validation, and test sets for the current period\n",
        "    train_indices = [j for j, year in enumerate(max_years) if year in train_years]\n",
        "    validation_indices = [j for j, year in enumerate(max_years) if year == validation_year]\n",
        "    test_indices = [j for j, year in enumerate(max_years) if year == test_year]\n",
        "\n",
        "    # Store them in the data_splits dictionary\n",
        "    for name, data_list in zip(data_list_names, data_lists):\n",
        "        data_splits[i]['train'][name] = [data_list[j] for j in train_indices]\n",
        "        data_splits[i]['validation'][name] = [data_list[j] for j in validation_indices]\n",
        "        data_splits[i]['test'][name] = [data_list[j] for j in test_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiyYGwDtJjfe"
      },
      "source": [
        "# Fit Models and Predict (Constituents Dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code segments implement the models for prediction as defined in the thesis paper. <br> Subsequently, they are used to predict on the **Constituents Dataset**."
      ],
      "metadata": {
        "id": "u-VGnfPBmMzX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgFu09VzMQdM"
      },
      "outputs": [],
      "source": [
        "# Reshape Data for CNN input\n",
        "data_splits_reshaped = {}\n",
        "\n",
        "for period_key, period_data in data_splits.items():\n",
        "    reshaped_period_data = {}\n",
        "\n",
        "    for split_key, split_data in period_data.items():\n",
        "        reshaped_split_data = {}\n",
        "\n",
        "        for method_key, method_data in split_data.items():\n",
        "            if method_key == 'raw_time_series':\n",
        "                reshaped_data = [array.reshape(20, 1, 1) for array in method_data]\n",
        "            elif method_key in ['candlestick', 'mtf', 'gaf']:\n",
        "                reshaped_data = [array.reshape(20, 20, 1) for array in method_data]\n",
        "            else:  # For 'labels', no reshaping is needed\n",
        "                reshaped_data = method_data\n",
        "\n",
        "            reshaped_split_data[method_key] = reshaped_data\n",
        "\n",
        "        reshaped_period_data[split_key] = reshaped_split_data\n",
        "\n",
        "    data_splits_reshaped[period_key] = reshaped_period_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convolutional Neural Network (CNN)**"
      ],
      "metadata": {
        "id": "v-58ZN17l15_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lKjqPSIN2IC"
      },
      "outputs": [],
      "source": [
        "def create_raw_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(256, (3, 1), activation='relu', padding='same', input_shape=(20, 1, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(256, (3, 1), activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(512, (3, 1), activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(512, (3, 1), activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(1024, (3, 1), activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(1024, (3, 1), activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(8192, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(8192, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    learning_rate = 1e-5\n",
        "    optimizer = Adam(learning_rate=learning_rate) # tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_image_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same', input_shape=(20, 20, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(1024, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(1024, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(8192, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(8192, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    learning_rate = 1e-5\n",
        "    optimizer = Adam(learning_rate=learning_rate) # tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qoApHnqEh8A"
      },
      "outputs": [],
      "source": [
        "def train_predict_evaluate(X_train, X_validation, X_test, y_train, y_validation, y_test, method):\n",
        "    if method == 'raw_time_series':\n",
        "        model = create_raw_model()\n",
        "    else:\n",
        "        model = create_image_model()\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)  # Stop training if validation loss doesn't decrease for 3 epochs\n",
        "    model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)  # Save the model with the lowest validation loss\n",
        "\n",
        "    model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_validation, y_validation), callbacks=[early_stopping, model_checkpoint])  # Pass the callbacks list to the fit method\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "    true_labels = np.argmax(y_test, axis=1)  # Convert the one-hot encoded y_test to label indices\n",
        "\n",
        "    loss = model.evaluate(X_test, y_test, verbose=0)[0]\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "    print(f\"Test loss for {method}: {loss}\")\n",
        "    print(f\"Test accuracy for {method}: {accuracy}\")\n",
        "    return loss, accuracy, model\n",
        "\n",
        "methods = ['raw_time_series', 'candlestick', 'mtf', 'gaf']\n",
        "\n",
        "def process_period_cnn(period_data):\n",
        "    period_results = {}\n",
        "    period_models_fitted = {}\n",
        "\n",
        "    for method in methods:\n",
        "        X_train = np.array(period_data['train'][method])\n",
        "        y_train = to_categorical(np.array(period_data['train']['labels']))\n",
        "        X_validation = np.array(period_data['validation'][method])\n",
        "        y_validation = to_categorical(np.array(period_data['validation']['labels']))\n",
        "        X_test = np.array(period_data['test'][method])\n",
        "        y_test = to_categorical(np.array(period_data['test']['labels']))\n",
        "\n",
        "        loss, accuracy, model = train_predict_evaluate(X_train, X_validation, X_test, y_train, y_validation, y_test, method)\n",
        "\n",
        "        period_results[method] = {'loss': loss, 'accuracy': accuracy}\n",
        "        period_models_fitted[method] = model\n",
        "\n",
        "    return period_results, period_models_fitted\n",
        "\n",
        "# period_results, period_models_fitted = process_period_cnn(data_splits_reshaped[5])\n",
        "\n",
        "# for name, model in period_models_fitted.items():\n",
        "    # model.save(f'/content/drive/My Drive/X_{name}_model_period_6.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Moving Average (SMA)**"
      ],
      "metadata": {
        "id": "6PBJCembl-kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def average_calculation(raw_time_series_arrays, labels):\n",
        "    average_values = []\n",
        "    average_labels = []\n",
        "\n",
        "    for raw_time_series in raw_time_series_arrays:\n",
        "        if len(raw_time_series) != 20:\n",
        "            raise ValueError(\"Each array in the input list must contain exactly 20 elements.\")\n",
        "\n",
        "        # Calculate the average for the all 20 elements\n",
        "        avg_value = sum(raw_time_series) / len(raw_time_series)\n",
        "\n",
        "        # Determine the label (1 if the 20th element of the time series is > thatn the average, 0 otherwise)\n",
        "        label = 1 if raw_time_series[-1] > avg_value else 0\n",
        "\n",
        "        average_values.append(avg_value)\n",
        "        average_labels.append(label)\n",
        "\n",
        "    # Calculate accuracy for the period\n",
        "    accuracy = accuracy_score(labels, average_labels)\n",
        "\n",
        "    return average_values, average_labels, accuracy\n",
        "\n",
        "def process_periods_avg(data_splits_reshaped):\n",
        "    period_dict = {}\n",
        "\n",
        "    for period_key, period_value in data_splits_reshaped.items():\n",
        "        test_split_value = period_value['test']\n",
        "        raw_time_series = test_split_value['raw_time_series']\n",
        "        labels = test_split_value['labels']\n",
        "\n",
        "        avg_values, avg_labels, accuracy = average_calculation(raw_time_series, labels)\n",
        "\n",
        "        period_dict[f'period_{period_key}_test'] = {\n",
        "            'average_values': avg_values,\n",
        "            'average_labels': avg_labels,\n",
        "            'accuracy': accuracy,\n",
        "        }\n",
        "\n",
        "    return period_dict\n",
        "\n",
        "# all_periods_average_data = process_periods_avg(data_splits_reshaped)\n",
        "\n",
        "# for period, period_data in all_periods_average_data.items():\n",
        "    # print(f\"Accuracy for {period}: {period_data['accuracy']}\")"
      ],
      "metadata": {
        "id": "cPZVzm7J00DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Prediction (RND)**"
      ],
      "metadata": {
        "id": "lP8z9pbAmBss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_periods_rnd(data_splits_reshaped, num_iterations=5):\n",
        "    period_dict = {}\n",
        "\n",
        "    for period_key, period_value in data_splits_reshaped.items():\n",
        "        np.random.seed(0)  # Set the random seed\n",
        "        test_split_value = period_value['test']\n",
        "        labels = test_split_value['labels']\n",
        "        labels_one_hot = to_categorical(labels)\n",
        "\n",
        "        # Iterate over the number of requested iterations\n",
        "        for i in range(num_iterations):\n",
        "            # Generate random labels\n",
        "            random_labels = np.random.randint(0, 2, size=(len(labels),))\n",
        "            random_probabilities = tf.nn.softmax(np.random.rand(len(labels), 2)).numpy()\n",
        "\n",
        "            # Calculate accuracy for the period\n",
        "            accuracy = accuracy_score(labels, random_labels)\n",
        "\n",
        "            # Calculate the categorical cross entropy loss\n",
        "            loss = categorical_crossentropy(labels_one_hot, random_probabilities).numpy().mean()\n",
        "\n",
        "            if period_key not in period_dict:\n",
        "                period_dict[period_key] = {}\n",
        "\n",
        "            period_dict[period_key][f'iteration_{i+1}'] = {\n",
        "                'random_labels': random_labels,\n",
        "                'random_probabilities': random_probabilities,\n",
        "                'accuracy': accuracy,\n",
        "                'loss': loss\n",
        "            }\n",
        "\n",
        "    return period_dict\n",
        "\n",
        "# all_periods_random_data = process_periods_rnd(data_splits_reshaped)\n",
        "\n",
        "# for period, period_data in all_periods_random_data.items():\n",
        "#    for iteration, iteration_data in period_data.items():\n",
        "#        print(f\"Loss for {period} during {iteration}: {iteration_data['loss']}\")\n",
        "#        print(f\"Accuracy for {period} during {iteration}: {iteration_data['accuracy']}\")"
      ],
      "metadata": {
        "id": "7-il7bmuGNar"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "Qh4hTrpgPKYE"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}